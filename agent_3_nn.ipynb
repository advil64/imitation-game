{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List that holds all the grids to be used as inputs for training\n",
    "x_train_grid = []\n",
    "# List that holds all the positions to be used as inputs for training\n",
    "x_train_position = []\n",
    "# List that holds all the senses to be used as inputs for training\n",
    "x_train_sense = []\n",
    "# List that holds all the locals to be used as inputs for training\n",
    "x_train_locals = []\n",
    "# List that holds all the directions to be used as outputs for training\n",
    "y_train = []\n",
    "\n",
    "# List that holds all the grids to be used as inputs for testing\n",
    "x_test_grid = []\n",
    "# List that holds all the positions to be used as inputs for testing\n",
    "x_test_position = []\n",
    "# List that holds all the sense to be used as inputs for testing\n",
    "x_test_sense = []\n",
    "# List that holds all the locals to be used as inputs for testing\n",
    "x_test_locals = []\n",
    "# List that holds all the directions to be used as outputs for testing\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of directory for grids\n",
    "directory_name = './data/big_3/'\n",
    "\n",
    "direct_count = [0 for i in range(4)]\n",
    "\n",
    "# iterate through all training grids\n",
    "for file_name in os.listdir(directory_name):\n",
    "    f = open(directory_name + file_name)\n",
    "    try:\n",
    "        data = json.load(f)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    # Iterate through all the data in a given grid and append their input and output values\n",
    "    for i in data:\n",
    "        if len(i['gridworld']) != 100:\n",
    "            print(file_name)\n",
    "            continue\n",
    "        x_train_grid.append(i['gridworld'])\n",
    "        x_train_position.append(i['position'])\n",
    "        x_train_sense.append(i['sense'])\n",
    "        x_train_locals.append(i['local'])\n",
    "        y_train.append(i['direction'])\n",
    "        direct_count[i['direction']] += 1\n",
    "    \n",
    "    # Close file socket\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[759, 2949, 11111, 14882]\n"
     ]
    }
   ],
   "source": [
    "print(direct_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of directory for grids\n",
    "directory_name = './data/big_3test/'\n",
    "\n",
    "# iterate through all training grids\n",
    "for file_name in os.listdir(directory_name):\n",
    "    f = open(directory_name + file_name)\n",
    "    data = json.load(f)\n",
    "\n",
    "    # Iterate through all the data in a given grid and append their input and output values\n",
    "    for i in data:\n",
    "        x_test_grid.append(i['gridworld'])\n",
    "        x_test_position.append(i['position'])\n",
    "        x_test_sense.append(i['sense'])\n",
    "        x_test_locals.append(i['local'])\n",
    "        y_test.append(i['direction'])\n",
    "    \n",
    "    # Close file socket\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "train_in_grid = np.reshape( x_train_grid, (-1, 10, 10) ) / 2\n",
    "train_in_position = np.reshape( x_train_position, (-1, 10, 10) )\n",
    "train_in_sense = np.reshape( x_train_sense, (-1, 10, 10) ) / 8\n",
    "train_in_locals = np.reshape( x_train_locals, (-1, 5, 5) )\n",
    "train_out = tf.keras.utils.to_categorical( y_train, 4 )\n",
    "\n",
    "test_in_grid = np.reshape( x_test_grid, (-1, 10, 10) ) / 2\n",
    "test_in_position = np.reshape( x_test_position, (-1, 10, 10) )\n",
    "test_in_sense = np.reshape( x_test_sense, (-1, 10, 10) ) / 8\n",
    "test_in_locals = np.reshape( x_test_locals, (-1, 5, 5) )\n",
    "test_out = tf.keras.utils.to_categorical( y_test, 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the grid input\n",
    "grid_input = tf.keras.layers.Input( shape = (10,10) )\n",
    "flatten_grid = tf.keras.layers.Flatten()( grid_input )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the position input\n",
    "position_input = tf.keras.layers.Input( shape = (10,10) )\n",
    "flatten_position = tf.keras.layers.Flatten()( position_input )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the sense input\n",
    "sense_input = tf.keras.layers.Input( shape = (10,10) )\n",
    "flatten_sense = tf.keras.layers.Flatten()( sense_input )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the local inputs\n",
    "local_input = tf.keras.layers.Input( shape = (5,5) )\n",
    "flatten_local = tf.keras.layers.Flatten()( local_input )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 325), dtype=tf.float32, name=None), name='concatenate_7/concat:0', description=\"created by layer 'concatenate_7'\")\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the grid and position into one vector which will be passed to neural network as input\n",
    "final_input = tf.keras.layers.Concatenate()([flatten_grid, flatten_position, flatten_sense, flatten_local])\n",
    "print(final_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create layers for Neural Network\n",
    "dense_1 = tf.keras.layers.Dense( units = 64, activation = tf.nn.relu )( final_input )\n",
    "dense_2 = tf.keras.layers.Dense( units = 32, activation = tf.nn.relu )( dense_1 )\n",
    "dense_3 = tf.keras.layers.Dense( units = 16, activation = tf.nn.relu )( dense_2 )\n",
    "dense_4 = tf.keras.layers.Dense( units = 8, activation = tf.nn.relu )( dense_3 )\n",
    "logits = tf.keras.layers.Dense( units = 4, activation = None )( dense_4 )\n",
    "probabilities = tf.keras.layers.Softmax()( logits )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the neural network to use stochastic gradient descent as the optimizer and categorical_crossentropy as loss function\n",
    "model = tf.keras.Model( inputs = [grid_input, position_input, sense_input, local_input], outputs = probabilities )\n",
    "opt = tf.keras.optimizers.SGD()\n",
    "model.compile( optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix( data, labels ):\n",
    "    mat = [ [ 0 for i in range(4) ] for j in range(4) ]\n",
    "    \n",
    "    prob_predict = model.predict( data )\n",
    "    #print(prob_predict)\n",
    "    predictions = np.argmax( prob_predict, axis = 1 )\n",
    "    \n",
    "    for i in range( data[0].shape[0] ):\n",
    "        mat[ labels[i] ][ predictions[i] ] += 1\n",
    "        # if labels[i] == 0:\n",
    "        #     print(prob_predict[i])\n",
    "    \n",
    "    for i in range(4):\n",
    "        print( \"\\t\".join( [ str(c) for c in mat[i] ] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\t39\t9\t0\n",
      "57\t164\t76\t8\n",
      "285\t388\t289\t138\n",
      "163\t613\t688\t0\n"
     ]
    }
   ],
   "source": [
    "# Test out before training\n",
    "generate_confusion_matrix( [test_in_grid, test_in_position, test_in_sense, test_in_locals], y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "929/929 [==============================] - 4s 3ms/step - loss: 0.4424 - accuracy: 0.8525 - val_loss: 0.2883 - val_accuracy: 0.9016\n",
      "Epoch 2/20\n",
      "929/929 [==============================] - 2s 3ms/step - loss: 0.2700 - accuracy: 0.9063 - val_loss: 0.2899 - val_accuracy: 0.9071\n",
      "Epoch 3/20\n",
      "929/929 [==============================] - 3s 3ms/step - loss: 0.2329 - accuracy: 0.9178 - val_loss: 0.1986 - val_accuracy: 0.9372\n",
      "Epoch 4/20\n",
      "929/929 [==============================] - 3s 3ms/step - loss: 0.2082 - accuracy: 0.9291 - val_loss: 0.1831 - val_accuracy: 0.9467\n",
      "Epoch 5/20\n",
      "929/929 [==============================] - 2s 3ms/step - loss: 0.1885 - accuracy: 0.9369 - val_loss: 0.1656 - val_accuracy: 0.9498\n",
      "Epoch 6/20\n",
      "929/929 [==============================] - 2s 3ms/step - loss: 0.1758 - accuracy: 0.9404 - val_loss: 0.1597 - val_accuracy: 0.9522\n",
      "Epoch 7/20\n",
      "929/929 [==============================] - 2s 2ms/step - loss: 0.1655 - accuracy: 0.9450 - val_loss: 0.1553 - val_accuracy: 0.9505\n",
      "Epoch 8/20\n",
      "929/929 [==============================] - 3s 3ms/step - loss: 0.1576 - accuracy: 0.9474 - val_loss: 0.1467 - val_accuracy: 0.9559\n",
      "Epoch 9/20\n",
      "929/929 [==============================] - 2s 3ms/step - loss: 0.1481 - accuracy: 0.9502 - val_loss: 0.1459 - val_accuracy: 0.9536\n",
      "Epoch 10/20\n",
      "929/929 [==============================] - 2s 3ms/step - loss: 0.1425 - accuracy: 0.9513 - val_loss: 0.1526 - val_accuracy: 0.9491\n",
      "Epoch 11/20\n",
      "929/929 [==============================] - 3s 3ms/step - loss: 0.1372 - accuracy: 0.9534 - val_loss: 0.1406 - val_accuracy: 0.9580\n",
      "Epoch 12/20\n",
      "929/929 [==============================] - 3s 3ms/step - loss: 0.1325 - accuracy: 0.9544 - val_loss: 0.1393 - val_accuracy: 0.9573\n",
      "Epoch 13/20\n",
      "929/929 [==============================] - 3s 3ms/step - loss: 0.1333 - accuracy: 0.9548 - val_loss: 0.1342 - val_accuracy: 0.9566\n",
      "Epoch 14/20\n",
      "929/929 [==============================] - 3s 3ms/step - loss: 0.1235 - accuracy: 0.9577 - val_loss: 0.1317 - val_accuracy: 0.9587\n",
      "Epoch 15/20\n",
      "929/929 [==============================] - 3s 3ms/step - loss: 0.1210 - accuracy: 0.9576 - val_loss: 0.1374 - val_accuracy: 0.9577\n",
      "Epoch 16/20\n",
      "929/929 [==============================] - 3s 3ms/step - loss: 0.1181 - accuracy: 0.9587 - val_loss: 0.1371 - val_accuracy: 0.9583\n",
      "Epoch 17/20\n",
      "929/929 [==============================] - 3s 3ms/step - loss: 0.1165 - accuracy: 0.9591 - val_loss: 0.1259 - val_accuracy: 0.9604\n",
      "Epoch 18/20\n",
      "929/929 [==============================] - 3s 3ms/step - loss: 0.1127 - accuracy: 0.9609 - val_loss: 0.2599 - val_accuracy: 0.9013\n",
      "Epoch 19/20\n",
      "929/929 [==============================] - 3s 3ms/step - loss: 0.1095 - accuracy: 0.9611 - val_loss: 0.1284 - val_accuracy: 0.9614\n",
      "Epoch 20/20\n",
      "929/929 [==============================] - 3s 3ms/step - loss: 0.1074 - accuracy: 0.9635 - val_loss: 0.1332 - val_accuracy: 0.9597\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit( [train_in_grid, train_in_position, train_in_sense, train_in_locals], train_out, validation_data=([test_in_grid, test_in_position, test_in_sense, test_in_locals], test_out), epochs = 20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\t26\t6\t7\n",
      "3\t255\t41\t6\n",
      "0\t9\t1080\t11\n",
      "0\t1\t8\t1455\n"
     ]
    }
   ],
   "source": [
    "# Test out after training\n",
    "generate_confusion_matrix( [test_in_grid, test_in_position, test_in_sense, test_in_locals], y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/agent3_NN/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./models/agent3_NN')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
